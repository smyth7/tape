{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at adding_model.py and setup.py\n",
    "\n",
    "setup.py: One entry point is 'tape-embed = tape.main:run_embed' > training.run_embed: \n",
    "\n",
    "```\n",
    "dataset = task_spec.dataset(data_file, tokenizer=tokenizer) > \n",
    "valid_loader = utils.setup_loader(dataset, batch_size, local_rank, n_gpu, 1, num_workers)\n",
    "```\n",
    "\n",
    "Ended up at tape.datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<function tape.datasets.dataset_factory(data_file: Union[str, pathlib.Path], *args, **kwargs) -> torch.utils.data.dataset.Dataset>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from tape import datasets\n",
    "datasets.dataset_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprot = datasets.dataset_factory(data_file='../data/uniprot_sprot.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'id': 'sp|Q6GZX3|002L_FRG3G',\n 'primary': 'MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQTCASGFCTSQPLCARIKKTQVCGLRYSSKGKDPLVSAEWDSRGAPYVRCTYDADLIDTQAQVDQFVSMFGESPSLAERYCMRGVKNTAGELVSRVSSDADPAGGWCRKWYSAHRGPDQDAALGSFCIKNPGAADCKCINRASDPVYQKVKTLHAYPDQCWYVPCAADVGELKMGTQRDTPTNCPTQVCQIVFNMLDDGSVTMDDVKNTINCDFSKYVPPPPPPKPTPPTPPTPPTPPTPPTPPTPPTPRPVHNRKVMFFVAGAVLVAILISTVRW',\n 'protein_length': 320}"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "sprot[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tape import ProteinOneHotModel, TAPETokenizer, ProteinLSTMModel, ProteinLSTMConfig\n",
    "tokenizer = TAPETokenizer(vocab='iupac')  # iupac is the vocab for TAPE models, use unirep for the UniRep model\n",
    "config=ProteinLSTMConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{\n  \"finetuning_task\": null,\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"input_size\": 128,\n  \"num_hidden_layers\": 3,\n  \"num_labels\": 2,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"torchscript\": false,\n  \"vocab_size\": 30\n}"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ProteinLSTMModel(config= config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ProteinLSTMModel(\n  (embed_matrix): Embedding(30, 128)\n  (encoder): ProteinLSTMEncoder(\n    (forward_lstm): ModuleList(\n      (0): ProteinLSTMLayer(\n        (dropout): Dropout(p=0.0, inplace=False)\n        (lstm): LSTM(128, 1024, batch_first=True)\n      )\n      (1): ProteinLSTMLayer(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (lstm): LSTM(1024, 1024, batch_first=True)\n      )\n      (2): ProteinLSTMLayer(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (lstm): LSTM(1024, 1024, batch_first=True)\n      )\n    )\n    (reverse_lstm): ModuleList(\n      (0): ProteinLSTMLayer(\n        (dropout): Dropout(p=0.0, inplace=False)\n        (lstm): LSTM(128, 1024, batch_first=True)\n      )\n      (1): ProteinLSTMLayer(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (lstm): LSTM(1024, 1024, batch_first=True)\n      )\n      (2): ProteinLSTMLayer(\n        (dropout): Dropout(p=0.1, inplace=False)\n        (lstm): LSTM(1024, 1024, batch_first=True)\n      )\n    )\n  )\n  (pooler): ProteinLSTMPooler(\n    (scalar_reweighting): Linear(in_features=6, out_features=1, bias=True)\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 2, 11,  7, 23, 25,  9,  8, 21,  7, 15, 13, 11, 16, 11,  5, 13, 15, 15,\n         17, 11,  7, 25, 13, 11, 22, 11, 22, 15, 25,  5,  5, 11,  5, 15, 13, 23,\n         20,  3]])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Pfam Family: Hexapep, Clan: CL0536\n",
    "sequence = 'GCTVEDRCLIGMGAILLNGCVIGSGSLVAAGALITQ'\n",
    "token_ids = torch.tensor([tokenizer.encode(sequence)])\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(token_ids)\n",
    "sequence_output = output[0]\n",
    "pooled_output = output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[ 0.0110, -0.0019, -0.0101,  ..., -0.0172, -0.0082, -0.0149],\n         [ 0.0172, -0.0028, -0.0153,  ..., -0.0170, -0.0087, -0.0157],\n         [ 0.0209, -0.0033, -0.0185,  ..., -0.0183, -0.0081, -0.0160],\n         ...,\n         [ 0.0246, -0.0063, -0.0225,  ..., -0.0184, -0.0034, -0.0124],\n         [ 0.0248, -0.0057, -0.0221,  ..., -0.0166, -0.0016, -0.0125],\n         [ 0.0241, -0.0060, -0.0226,  ..., -0.0103,  0.0001, -0.0086]]],\n       grad_fn=<CatBackward>)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 1.0810e-03,  9.1288e-06,  3.6752e-04,  ..., -3.5943e-04,\n          5.1847e-04, -9.5619e-04]], grad_fn=<TanhBackward>)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{\n  \"finetuning_task\": null,\n  \"initializer_range\": 0.02,\n  \"num_labels\": 2,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"torchscript\": false,\n  \"use_evolutionary\": false,\n  \"vocab_size\": 30\n}"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from tape import ProteinOneHotModel, ProteinOneHotConfig\n",
    "tokenizer = TAPETokenizer(vocab='iupac')  # iupac is the vocab for TAPE models, use unirep for the UniRep model\n",
    "config=ProteinOneHotConfig(vocab_size= 30) # note I need to  specify the vocab size here\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ProteinOneHotModel()"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model=ProteinOneHotModel(config = config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(token_ids)\n",
    "sequence_output = output[0]\n",
    "pooled_output = output[1] # probably doesn't make sense here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.0000, 0.0000, 0.0263, 0.0263, 0.0000, 0.1053, 0.0000, 0.0789, 0.0263,\n         0.0263, 0.0000, 0.1842, 0.0000, 0.1053, 0.0000, 0.1316, 0.0263, 0.0263,\n         0.0000, 0.0000, 0.0263, 0.0263, 0.0526, 0.0526, 0.0000, 0.0789, 0.0000,\n         0.0000, 0.0000, 0.0000]])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([[[0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n tensor([[0.0000, 0.0000, 0.0263, 0.0263, 0.0000, 0.1053, 0.0000, 0.0789, 0.0263,\n          0.0263, 0.0000, 0.1842, 0.0000, 0.1053, 0.0000, 0.1316, 0.0263, 0.0263,\n          0.0000, 0.0000, 0.0263, 0.0263, 0.0526, 0.0526, 0.0000, 0.0789, 0.0000,\n          0.0000, 0.0000, 0.0000]]))"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittapeconda899adf3d7264499bab1aac0192c5e209",
   "display_name": "Python 3.7.7 64-bit ('tape': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}